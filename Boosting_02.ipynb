{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3dbd5bf4-3791-4e31-9cdc-a3af11a6c68e",
      "metadata": {
        "id": "3dbd5bf4-3791-4e31-9cdc-a3af11a6c68e"
      },
      "source": [
        "# Boosting 02"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f06dbd7-dfe6-4186-a2a8-a4427cc439ac",
      "metadata": {
        "id": "9f06dbd7-dfe6-4186-a2a8-a4427cc439ac"
      },
      "source": [
        "______"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71bceaca-70d6-4182-addc-e5b3cd876934",
      "metadata": {
        "id": "71bceaca-70d6-4182-addc-e5b3cd876934"
      },
      "source": [
        "# Q1. What is Gradient Boosting Regression?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient boosting regression is an ensable technique in boosting algorithems for solving the regression problem. In such a case the outputs are always be in a continues values. Gradient boosing regression also take the muliple models as base models to predict the final output.In the further section we tried to understand in a systamatic manner , that how a dradient boosting regression works.\n",
        "\n",
        "   **Weak learners :** In gradient boosting weak learners are the decision trees in most of the cases as base learners.These weak learners are called stupms in the gradient boosting as this algorithem only use the first 3 nodes of a decision tree.\n",
        "\n",
        "   **Arranging weak learners squentially :** The stumps are connected to each other in a squetial manner. and these stumps correcting the prediction , which is remains to corrected by previous weak learner. and this process will continue till the strong learner predicts the final outcome.\n",
        "\n",
        "   **Gradient Descent Optimization :** he term \"gradient\" in gradient boosting refers to the technique of gradient descent optimization used to minimize the errors. The algorithm minimizes the loss function by iteratively adjusting the parameters of the weak learners.\n",
        "\n",
        "   **Weight Comination :** The predictions of each weak learner are combined with a weighted sum, where the weights are determined during the training process. The weights are adjusted in the direction that minimizes the overall error.\n",
        "\n",
        "   **Regulazation :**  Gradient boosting often includes regularization techniques to prevent overfitting. Common regularization methods include controlling the learning rate, limiting the depth of the trees, and adding a penalty term to the loss function.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y2Zuu0VN3DPy"
      },
      "id": "y2Zuu0VN3DPy"
    },
    {
      "cell_type": "markdown",
      "id": "01086cc4-5b3d-41e1-9ed6-b2b2d418b1e0",
      "metadata": {
        "id": "01086cc4-5b3d-41e1-9ed6-b2b2d418b1e0"
      },
      "source": [
        "***Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bfa8c125-deb2-4694-a9c6-4753e5cf395f",
      "metadata": {
        "id": "bfa8c125-deb2-4694-a9c6-4753e5cf395f"
      },
      "outputs": [],
      "source": [
        "#importing important libraries\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score , mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3d7ba18e-9bca-4e23-9427-c1a94caa4203",
      "metadata": {
        "id": "3d7ba18e-9bca-4e23-9427-c1a94caa4203"
      },
      "outputs": [],
      "source": [
        "# defining the independent features and target\n",
        "X,y=make_regression(n_samples=1000,n_features=2,noise=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "054bf4de-fe38-4840-adee-3fe4f842d307",
      "metadata": {
        "id": "054bf4de-fe38-4840-adee-3fe4f842d307"
      },
      "outputs": [],
      "source": [
        "# Spliting the data into trainig data and test data\n",
        "X_train , X_test, y_train , y_test = train_test_split(X,y , test_size=0.33 , random_state= 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "97b291f9-404e-461b-923d-0db1154b9565",
      "metadata": {
        "id": "97b291f9-404e-461b-923d-0db1154b9565"
      },
      "outputs": [],
      "source": [
        "# initilizing the gradient boosting regressor\n",
        "gradient= GradientBoostingRegressor ()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f38b1305-52d6-4877-b8ec-dbe76b0b1fd1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "f38b1305-52d6-4877-b8ec-dbe76b0b1fd1",
        "outputId": "577dc553-0072-4072-a09b-e527f1dedfd0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# trinig the data by the model\n",
        "gradient.fit(X_train , y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "71ec7d38-a7c5-49cb-9a6a-5b77d046863a",
      "metadata": {
        "id": "71ec7d38-a7c5-49cb-9a6a-5b77d046863a"
      },
      "outputs": [],
      "source": [
        "# prediction for testing data\n",
        "y_pred = gradient.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e604d683-8541-48ab-9b3e-a91b14ded713",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e604d683-8541-48ab-9b3e-a91b14ded713",
        "outputId": "424f6e40-dc68-4497-b778-317e4bbfd6e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " R square score : 0.99928558110717\n",
            " Mean squared error : 6.584498674455702\n"
          ]
        }
      ],
      "source": [
        "# evaluating the performance of the model\n",
        "print (f\" R square score : {r2_score(y_test, y_pred)}\")\n",
        "print (f\" Mean squared error : { mean_squared_error(y_test , y_pred)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "795cc0a9-c356-4625-be31-6911e7dd05bf",
      "metadata": {
        "id": "795cc0a9-c356-4625-be31-6911e7dd05bf"
      },
      "source": [
        "***Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "95a23a2e-d9c5-4202-b08d-ada67e129bba",
      "metadata": {
        "id": "95a23a2e-d9c5-4202-b08d-ada67e129bba"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "443aedf7-4251-411f-bf73-6cc39b1fc6d1",
      "metadata": {
        "id": "443aedf7-4251-411f-bf73-6cc39b1fc6d1"
      },
      "outputs": [],
      "source": [
        "# defining the perameters\n",
        "parameters={'loss':['squared_error','absolute_error','huber','quantile'],\n",
        "           'learning_rate':[0.1,0.2,0.3,0.4,0.5],\n",
        "           'n_estimators':[100,200,300,400,500],\n",
        "           'criterion':['friedman_mse','squared_error']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a471e374-4fdb-4b10-9fe8-b1d5aac3cb85",
      "metadata": {
        "id": "a471e374-4fdb-4b10-9fe8-b1d5aac3cb85"
      },
      "outputs": [],
      "source": [
        "# intilizing the grid search cv\n",
        "cv= GridSearchCV( gradient , param_grid= parameters , cv= 2  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2767b72f-9ea1-4393-bf66-0b67c68fe4d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "id": "2767b72f-9ea1-4393-bf66-0b67c68fe4d8",
        "outputId": "2def3fba-4d5d-4a97-ab47-2d474a9460b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=2, estimator=GradientBoostingRegressor(),\n",
              "             param_grid={'criterion': ['friedman_mse', 'squared_error'],\n",
              "                         'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "                         'loss': ['squared_error', 'absolute_error', 'huber',\n",
              "                                  'quantile'],\n",
              "                         'n_estimators': [100, 200, 300, 400, 500]})"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=2, estimator=GradientBoostingRegressor(),\n",
              "             param_grid={&#x27;criterion&#x27;: [&#x27;friedman_mse&#x27;, &#x27;squared_error&#x27;],\n",
              "                         &#x27;learning_rate&#x27;: [0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "                         &#x27;loss&#x27;: [&#x27;squared_error&#x27;, &#x27;absolute_error&#x27;, &#x27;huber&#x27;,\n",
              "                                  &#x27;quantile&#x27;],\n",
              "                         &#x27;n_estimators&#x27;: [100, 200, 300, 400, 500]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=2, estimator=GradientBoostingRegressor(),\n",
              "             param_grid={&#x27;criterion&#x27;: [&#x27;friedman_mse&#x27;, &#x27;squared_error&#x27;],\n",
              "                         &#x27;learning_rate&#x27;: [0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "                         &#x27;loss&#x27;: [&#x27;squared_error&#x27;, &#x27;absolute_error&#x27;, &#x27;huber&#x27;,\n",
              "                                  &#x27;quantile&#x27;],\n",
              "                         &#x27;n_estimators&#x27;: [100, 200, 300, 400, 500]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# fiting the data , for which we are finding the best perameters\n",
        "cv.fit(X_train , y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1432bf34-f7fb-45f3-a740-351706f2192f",
      "metadata": {
        "id": "1432bf34-f7fb-45f3-a740-351706f2192f"
      },
      "outputs": [],
      "source": [
        "y_pred= cv.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2c3056a3-81b5-4967-8f90-1f40d627ca71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c3056a3-81b5-4967-8f90-1f40d627ca71",
        "outputId": "13fd1098-e781-44c8-9fd4-6d6e3726e14c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'criterion': 'friedman_mse',\n",
              " 'learning_rate': 0.1,\n",
              " 'loss': 'squared_error',\n",
              " 'n_estimators': 500}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# list of the best perametrs and their best values:\n",
        "\n",
        "best_params= cv.best_params_\n",
        "best_params"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# applying it to the gradient boosting model to see the perfromance of permameters\n",
        "\n",
        "grad = GradientBoostingRegressor (**best_params)\n",
        "\n",
        "#training the data with the assigned perametrs of gradient boosting\n",
        "grad.fit(X_train , y_train)\n",
        "\n",
        "#predicting the data\n",
        "y_pred = grad.predict(X_test)\n",
        "\n",
        "# evaluting the newscore after using best perameters\n",
        "\n",
        "print( f\" This is the r2 score after using best perameters : {r2_score(y_test, y_pred)}\")\n",
        "print ( f\" This is the MSE after using bestperameters : { mean_squared_error ( y_test, y_pred)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNwN-kX66p04",
        "outputId": "e69947b8-4155-4bc7-b74b-99d48d45c875"
      },
      "id": "rNwN-kX66p04",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " This is the r2 score after using best perameters : 0.9995136975437857\n",
            " This is the MSE after using bestperameters : 4.482045352472938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. What is a weak learner in Gradient Boosting?"
      ],
      "metadata": {
        "id": "d5da9afd-8fff-49bc-bb9e-1675a689f265"
      },
      "id": "d5da9afd-8fff-49bc-bb9e-1675a689f265"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the Gradient Boosting , the weak learner are usaully the decision trees. Which are appears as the stumps. Stumps are basically the vary basic part of decision trees as it appears only three nodes in a decision tree. A weak learner is a model that performs slightly better than random chance on a given task. As a individual tree has worked not that much potential but when these decision trees are combined to make a opinoin about the prediction of an certain dependent feature than it will more accurate than a individual decision tree as it combines the potential of multiple decision tree.\n",
        "\n",
        "**Limited Complexity:**  Weak learners are intentionally kept simple. In the case of decision trees, they are shallow and have a small number of nodes.\n",
        "\n",
        "**Sensitivity to Different Subsets of Data:** Weak learners should perform better than random chance but may not be robust across all subsets of the data.\n",
        "\n",
        "**Sequential Building:** In Gradient Boosting, weak learners are added sequentially. Each new weak learner focuses on the mistakes (residuals) made by the combination of the existing learners. This process allows the model to iteratively correct errors and improve predictive performance."
      ],
      "metadata": {
        "id": "JGfUZXTg8A6D"
      },
      "id": "JGfUZXTg8A6D"
    },
    {
      "cell_type": "markdown",
      "id": "32969b38-9498-48e2-95a8-0008393c793e",
      "metadata": {
        "id": "32969b38-9498-48e2-95a8-0008393c793e"
      },
      "source": [
        "# Q5. What is the intuition behind the Gradient Boosting algorithm?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ensemble learning involves combining multiple weak models to create a more robust and accurate model.\n",
        "- Gradient Boosting trains weak learners sequentially, with each one trying to correct the errors of the combined model (ensemble) built so far.\n",
        "- The algorithm minimizes the errors of the ensemble by adjusting the weights of individual weak learners.\n",
        "- A loss function is used to quantify the errors of the model. In regression problems, mean squared error (MSE) is commonly used, while classification problems often use log loss or exponential loss.\n",
        "- The final prediction is the sum of predictions from all the weak learners, each scaled by its learning rate."
      ],
      "metadata": {
        "id": "Q8EpP8fq123K"
      },
      "id": "Q8EpP8fq123K"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef07241b-14dc-47f0-b112-ab3f572de3e7",
      "metadata": {
        "id": "ef07241b-14dc-47f0-b112-ab3f572de3e7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "96677953-ea57-482b-8bc9-8137df06d792",
      "metadata": {
        "id": "96677953-ea57-482b-8bc9-8137df06d792"
      },
      "source": [
        "# Q. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constructing the mathematical intuition behind the Gradient Boosting algorithm involves understanding the key components and operations that drive the sequential training process.\n",
        "\n",
        "**Loss Function :** Start with a loss function that measures the error between the predicted values and the actual target values. For regression problems, the mean squared error (MSE) is commonly used, while classification problems may use log loss or exponential loss.\n",
        "\n",
        "**Initialize Model:** Initialize the model with a constant value, often the mean of the target variable for regression or the log-odds for classification.\n",
        "\n",
        "**Compute Residuals:** Calculate the residuals by subtracting the current predictions from the actual target values. Residuals represent the errors made by the current model.\n",
        "\n",
        "**Train Weak Learner:** Fit a weak learner (usually a decision tree) to the residuals. The goal is to create a model that can capture the patterns in the residuals and improve predictions\n",
        "\n",
        "**Compute Learning Rate Weight:** Introduce a learning rate (shrinkage factor) denoted by a small value (e.g., 0.01 or 0.1). This controls the step size of the updates to prevent overfitting and achieve smoother convergence.\n",
        "\n",
        "**Update Model:** Update the model by adding the product of the learning rate and the predictions of the newly trained weak learner. This update is performed to reduce the residuals and move closer to the actual target values.\n",
        "\n",
        "**Combine Predictions:** The final prediction is the sum of the predictions from all the weak learners, each multiplied by its learning rate. The combined model captures complex relationships and provides a more accurate prediction."
      ],
      "metadata": {
        "id": "WSr3nEUP3AfY"
      },
      "id": "WSr3nEUP3AfY"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}